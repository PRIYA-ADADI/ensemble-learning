{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Theory Answers**\n",
        "\n",
        "**Q1. Can we use Bagging for regression problems?**\n",
        "\n",
        "Yes, Bagging can be applied to regression problems. In regression, Bagging combines predictions from multiple regressors (like Decision Trees) by averaging their outputs. Each regressor is trained on a bootstrap sample of the dataset, which introduces diversity. This averaging reduces variance and improves generalization. Bagging is especially effective in reducing overfitting when base regressors are unstable, such as decision trees.\n",
        "\n",
        "\n",
        "**Q2. What is the difference between multiple model training and single model training?**\n",
        "\n",
        "Single model training involves building one model on the dataset and making predictions based on it. Multiple model training, as in ensemble methods, builds several models and combines their outputs. Multiple model training often reduces error and increases robustness because errors of individual models may cancel out. In contrast, a single model can overfit or underfit depending on its complexity. Thus, ensembles provide higher accuracy and stability compared to single models.\n",
        "\n",
        "\n",
        "\n",
        "**Q3. Explain the concept of feature randomness in Random Forest.**\n",
        "\n",
        "Feature randomness in Random Forest means that at each node split, only a random subset of features is considered rather than all features. This randomness ensures that different trees learn different patterns, making the forest more diverse. The result is reduced correlation among the trees, which improves overall accuracy. Feature randomness also helps avoid bias towards dominant features. It is one of the key tricks that makes Random Forest better than simple Bagging.\n",
        "\n",
        "\n",
        "\n",
        "**Q4. What is OOB (Out-of-Bag) Score?**\n",
        "\n",
        "The Out-of-Bag score is a built-in validation technique used in Bagging and Random Forest. Since each tree is trained on a bootstrap sample, some data points are left out, called Out-of-Bag samples. These OOB samples are used to test the model without needing a separate validation set. The OOB score provides an unbiased estimate of model performance. It is very efficient as it reuses training data for validation purposes.\n",
        "\n",
        "**Q5. How can you measure the importance of features in a Random Forest model?**\n",
        "\n",
        "Random Forest measures feature importance by evaluating how much each feature decreases impurity across all trees. One common method is the Gini Importance, which checks how often a feature is used for splitting and how much it reduces error. Another approach is permutation importance, where values of a feature are shuffled, and the drop in accuracy is measured. High importance means the feature contributes more to predictions. This helps in feature selection and model interpretation.\n",
        "\n",
        "\n",
        "\n",
        "**Q6. Explain the working principle of a Bagging Classifier.**\n",
        "\n",
        "The Bagging Classifier works by training multiple base classifiers on different bootstrap samples of the data. Each classifier gives a prediction, and the final output is decided by majority voting. Bagging reduces variance by averaging multiple independent models. Since each model sees slightly different data, the errors tend to cancel out. This method is highly effective when the base model is prone to overfitting, like decision trees.\n",
        "\n",
        "\n",
        "\n",
        "**Q7. How do you evaluate a Bagging Classifier’s performance?**\n",
        "\n",
        "The performance of a Bagging Classifier can be evaluated using metrics like accuracy, precision, recall, F1-score, and ROC-AUC, depending on the problem. Cross-validation can also be applied to check consistency across folds. Additionally, the Out-of-Bag (OOB) score can be used for validation without needing a separate test set. Comparing results with a single base model also highlights improvements. Visualization tools like confusion matrices further help assess classifier performance.\n",
        "\n",
        "\n",
        "\n",
        "**Q8. How does a Bagging Regressor work?**\n",
        "\n",
        "A Bagging Regressor works by training multiple base regressors (commonly decision trees) on different bootstrap samples. Each regressor makes predictions, and the final output is the average of all predictions. Averaging reduces variance and leads to more stable results. Since trees are sensitive to data variations, Bagging makes them more reliable. It works particularly well in noisy datasets and prevents overfitting compared to a single regressor.\n",
        "\n",
        "\n",
        "\n",
        "**Q9. What is the main advantage of ensemble techniques?**\n",
        "\n",
        "The main advantage of ensemble techniques is their ability to combine multiple models to achieve higher accuracy and robustness. They reduce both variance and bias compared to individual models. Ensembles can handle complex problems better by leveraging diverse models. They are also less sensitive to noise and outliers. Overall, ensemble methods provide better generalization on unseen data.\n",
        "\n",
        "\n",
        "\n",
        "**Q10. What is the main challenge of ensemble methods?**\n",
        "\n",
        "The main challenge of ensemble methods is their computational cost and complexity. Training multiple models requires more time and resources. They can also be harder to interpret compared to a single model, making explainability an issue. In some cases, ensembles risk overfitting if not properly tuned. Moreover, they may require careful parameter tuning, like the number of estimators or learning rate, to perform optimally. Despite these challenges, they often outperform individual models.\n",
        "\n",
        "\n",
        "\n",
        "**Q11. Explain the key idea behind ensemble techniques**\n",
        "\n",
        "The key idea of ensemble techniques is that a group of weak learners, when combined, can produce a stronger and more accurate model. Instead of depending on a single model, ensemble learning merges predictions from multiple models to reduce errors. This combination helps balance out biases and variances that individual models may have. The diversity among models makes ensembles more robust. By aggregating results, they can generalize better to unseen data. In essence, ensembles follow the principle that “many weak opinions can lead to a stronger decision.”\n",
        "\n",
        "\n",
        "\n",
        "**Q12. What is a Random Forest Classifier?**\n",
        "\n",
        "A Random Forest Classifier is an ensemble learning method based on decision trees. It builds multiple decision trees during training, each trained on a bootstrap sample with random feature selection at each split. For classification, the model outputs the class that has the majority vote across all trees. Random Forest reduces overfitting, which is common in single decision trees. It is highly effective in handling large datasets with many features. Random Forest is also widely used due to its interpretability through feature importance.\n",
        "\n",
        "\n",
        "**Q13. What are the main types of ensemble techniques?**\n",
        "\n",
        "The main types of ensemble techniques are Bagging, Boosting, and Stacking. Bagging (Bootstrap Aggregating) trains models in parallel and combines their outputs to reduce variance. Boosting, on the other hand, trains models sequentially, where each model learns from the errors of the previous one, reducing bias. Stacking involves training different models and then combining their outputs using a meta-model that learns how best to integrate them. Each method has its own strengths, but together, they cover a wide range of machine learning challenges.\n",
        "\n",
        "\n",
        "**Q14. What is ensemble learning in machine learning?**\n",
        "\n",
        "Ensemble learning is a technique in machine learning where multiple models, often called base learners, are trained and combined to solve the same problem. Instead of relying on a single predictor, ensemble learning aggregates their results to improve accuracy and robustness. These base learners could be the same type of models, such as decision trees, or different ones. The main goal is to achieve better performance and generalization. Ensemble learning has become a core approach in competitions and real-world applications due to its reliability.\n",
        "\n",
        "\n",
        "**Q15. When should we avoid using ensemble methods?**\n",
        "\n",
        "Ensemble methods should be avoided when interpretability is critical, as ensembles are often complex and hard to explain. They are also not suitable for small datasets since they may lead to overfitting. In cases where computational resources are limited, ensembles may not be ideal because they require more time and memory. If a single simple model provides satisfactory accuracy, then ensembles may be unnecessary. Furthermore, in real-time applications where speed is crucial, ensembles may be too slow for deployment.\n",
        "\n",
        "\n",
        "**Q16. How does Bagging help in reducing overfitting?**\n",
        "\n",
        "Bagging helps in reducing overfitting by training multiple models on different bootstrap samples and then combining their predictions. Overfitting happens when a model learns noise along with the data, but averaging across several models reduces this effect. Since each model sees slightly different data, their individual errors are less likely to align. The result is a smoother, more stable prediction. Bagging is particularly effective with unstable models like decision trees, which easily overfit to training data.\n",
        "\n",
        "\n",
        "\n",
        "**Q17. Why is Random Forest better than a single Decision Tree?**\n",
        "\n",
        "Random Forest is better than a single decision tree because it reduces overfitting and provides higher accuracy. A single decision tree can become too complex and learn noise from the training set, leading to poor generalization. Random Forest overcomes this by building multiple trees using bootstrap samples and random feature selection. The ensemble then takes majority voting, which balances errors and improves stability. This makes Random Forest more robust, less biased, and generally more accurate than a single tree.\n",
        "\n",
        "\n",
        "\n",
        "**Q18. What is the role of bootstrap sampling in Bagging?**\n",
        "\n",
        "Bootstrap sampling plays a crucial role in Bagging by providing diversity among models. In bootstrap sampling, each model is trained on a random sample taken with replacement from the original dataset. As a result, some data points are repeated while others are left out. This variation ensures that different models learn different patterns, making the ensemble less correlated. By combining these diverse models, Bagging reduces variance and improves overall performance. Bootstrap sampling is the foundation of Bagging’s effectiveness.\n",
        "\n",
        "\n",
        "\n",
        "**Q19. What are some real-world applications of ensemble techniques?**\n",
        "\n",
        "Ensemble techniques are widely used in various real-world applications. In finance, they are used for credit scoring, fraud detection, and stock market prediction. In healthcare, ensembles support disease diagnosis and patient risk assessment. In e-commerce, they power recommendation systems to improve user experience. They are also applied in natural language processing for tasks like sentiment analysis and spam detection. Additionally, ensembles dominate machine learning competitions like Kaggle because of their superior accuracy and robustness.\n",
        "\n",
        "\n",
        "**Q20. What is the difference between Bagging and Boosting?**\n",
        "\n",
        "Bagging and Boosting are both ensemble methods, but they differ in approach. Bagging trains models in parallel on bootstrap samples and combines results, mainly reducing variance. Boosting, however, trains models sequentially, where each new model corrects errors made by the previous one, reducing bias. Bagging gives equal weight to all models, while Boosting gives more weight to misclassified samples. Bagging is more robust against overfitting, while Boosting can achieve higher accuracy but may risk overfitting if not tuned properly.\n",
        "\n",
        "\n",
        "###**Practical Questions**\n",
        "\n",
        "**Q21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy**"
      ],
      "metadata": {
        "id": "eI2m4XdtFSYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging Classifier with Decision Tree\n",
        "bagging_clf = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Accuracy\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "print(\"Bagging Classifier Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHEiuyzeGXsN",
        "outputId": "89af7764-9642-4c6c-d594-cb44ea969e64"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)**"
      ],
      "metadata": {
        "id": "kCyNq6CAGbHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging Regressor\n",
        "bagging_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# MSE\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "print(\"Bagging Regressor MSE:\", mean_squared_error(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqZHjzCSGseJ",
        "outputId": "c2528f1f-3ee1-438c-ade1-86df9ef3c594"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 2987.0073593984966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores**"
      ],
      "metadata": {
        "id": "wwsTblnzGuAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Feature Importance\n",
        "importances = rf_clf.feature_importances_\n",
        "for name, importance in zip(load_breast_cancer().feature_names, importances):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BblxzjZ0G4uq",
        "outputId": "98155dc7-91da-4d7a-f63d-b48e13a19a41"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean radius: 0.0323\n",
            "mean texture: 0.0111\n",
            "mean perimeter: 0.0601\n",
            "mean area: 0.0538\n",
            "mean smoothness: 0.0062\n",
            "mean compactness: 0.0092\n",
            "mean concavity: 0.0806\n",
            "mean concave points: 0.1419\n",
            "mean symmetry: 0.0033\n",
            "mean fractal dimension: 0.0031\n",
            "radius error: 0.0164\n",
            "texture error: 0.0032\n",
            "perimeter error: 0.0118\n",
            "area error: 0.0295\n",
            "smoothness error: 0.0059\n",
            "compactness error: 0.0046\n",
            "concavity error: 0.0058\n",
            "concave points error: 0.0034\n",
            "symmetry error: 0.0040\n",
            "fractal dimension error: 0.0071\n",
            "worst radius: 0.0780\n",
            "worst texture: 0.0188\n",
            "worst perimeter: 0.0743\n",
            "worst area: 0.1182\n",
            "worst smoothness: 0.0118\n",
            "worst compactness: 0.0175\n",
            "worst concavity: 0.0411\n",
            "worst concave points: 0.1271\n",
            "worst symmetry: 0.0129\n",
            "worst fractal dimension: 0.0069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q24. Train a Random Forest Regressor and compare its performance with a single Decision Tree**"
      ],
      "metadata": {
        "id": "B-cdPc_zG7Ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Decision Tree\n",
        "dt = DecisionTreeRegressor(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_mse = mean_squared_error(y_test, dt_pred)\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "print(\"Decision Tree MSE:\", dt_mse)\n",
        "print(\"Random Forest MSE:\", rf_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxqCs079G-_c",
        "outputId": "22a70ec1-8ec1-43ab-86e1-5376df22c1b1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree MSE: 0.5280096503174904\n",
            "Random Forest MSE: 0.25650512920799395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier**"
      ],
      "metadata": {
        "id": "-lbRTxbyHA_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Random Forest with OOB score\n",
        "rf_clf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    oob_score=True,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"OOB Score:\", rf_clf.oob_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDhdCGk6HFxd",
        "outputId": "8717492f-edcd-4c89-ba2d-3fd01df1404d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOB Score: 0.9838709677419355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q26. Train a Bagging Classifier using SVM as a base estimator and print accuracy**"
      ],
      "metadata": {
        "id": "_LZFVaN-HHqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging with SVM\n",
        "bagging_svm = BaggingClassifier(\n",
        "    estimator=SVC(probability=True),\n",
        "    n_estimators=20,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_svm.fit(X_train, y_train)\n",
        "\n",
        "# Accuracy\n",
        "y_pred = bagging_svm.predict(X_test)\n",
        "print(\"Bagging with SVM Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrvWWVCpHNNj",
        "outputId": "d39ebf8e-ac43-4d7e-da06-546e0b15b53f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging with SVM Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q27. Train a Random Forest Classifier with different numbers of trees and compare accuracy**"
      ],
      "metadata": {
        "id": "R9geZ8vMHTyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Compare Random Forest with different n_estimators\n",
        "for n in [10, 50, 100, 200]:\n",
        "    rf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    print(f\"Random Forest (n_estimators={n}) Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxvGaEAZHYZL",
        "outputId": "68e69834-76f3-4e82-a5ce-887a3cff66ea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest (n_estimators=10) Accuracy: 0.9259259259259259\n",
            "Random Forest (n_estimators=50) Accuracy: 1.0\n",
            "Random Forest (n_estimators=100) Accuracy: 1.0\n",
            "Random Forest (n_estimators=200) Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score**"
      ],
      "metadata": {
        "id": "cX8pnlQlHaS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging with Logistic Regression\n",
        "bagging_lr = BaggingClassifier(\n",
        "    estimator=LogisticRegression(max_iter=1000),\n",
        "    n_estimators=20,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_lr.fit(X_train, y_train)\n",
        "\n",
        "# AUC Score\n",
        "y_pred_prob = bagging_lr.predict_proba(X_test)[:, 1]\n",
        "print(\"Bagging with Logistic Regression AUC:\", roc_auc_score(y_test, y_pred_prob))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrbjSmJ0Hc_7",
        "outputId": "bb51b196-c848-4749-f073-b727ea39bbc3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging with Logistic Regression AUC: 0.9979423868312758\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q29. Train a Random Forest Regressor and analyze feature importance scores**"
      ],
      "metadata": {
        "id": "y4NA16nMHfy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Load dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "# Feature Importances\n",
        "importances = rf_reg.feature_importances_\n",
        "for name, importance in zip(fetch_california_housing().feature_names, importances):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7A6ejncHmBz",
        "outputId": "d56cb2b0-0fd5-4779-8057-f86ad1e69c9b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MedInc: 0.5260\n",
            "HouseAge: 0.0547\n",
            "AveRooms: 0.0472\n",
            "AveBedrms: 0.0300\n",
            "Population: 0.0317\n",
            "AveOccup: 0.1382\n",
            "Latitude: 0.0861\n",
            "Longitude: 0.0861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q30. Train an ensemble model using both Bagging and Random Forest and compare accuracy**"
      ],
      "metadata": {
        "id": "rIS9mlMeHqtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging Classifier\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "bag_acc = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "rf_acc = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Bagging Classifier Accuracy:\", bag_acc)\n",
        "print(\"Random Forest Classifier Accuracy:\", rf_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0dK_uB3HtWq",
        "outputId": "6b075b32-db3e-4c81-b40a-993e9ee09c82"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 1.0\n",
            "Random Forest Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV**"
      ],
      "metadata": {
        "id": "vTbdAXWoH8wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# GridSearchCV for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "grid = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQKTUt5QII38",
        "outputId": "e7e16e96-8425-40ed-9218-c8c31084d44a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "Best Accuracy: 0.968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q32. Train a Bagging Regressor with different numbers of base estimators and compare performance**"
      ],
      "metadata": {
        "id": "UyCU_GrCIOX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Compare different n_estimators\n",
        "for n in [10, 50, 100, 200]:\n",
        "    bagging_reg = BaggingRegressor(\n",
        "        estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=n,\n",
        "        random_state=42\n",
        "    )\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Bagging Regressor (n_estimators={n}) MSE:\", mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGCEghQ8ITpr",
        "outputId": "8faaab20-68ef-4f7a-bfed-f668f5e5c4bc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor (n_estimators=10) MSE: 0.28623579601385674\n",
            "Bagging Regressor (n_estimators=50) MSE: 0.25787382250585034\n",
            "Bagging Regressor (n_estimators=100) MSE: 0.2568358813508342\n",
            "Bagging Regressor (n_estimators=200) MSE: 0.2541650541215747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q33. Train a Random Forest Classifier and analyze misclassified samples**"
      ],
      "metadata": {
        "id": "E_oQUzcfIVjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Misclassified samples\n",
        "misclassified = [(i, y_test[i], y_pred[i]) for i in range(len(y_test)) if y_test[i] != y_pred[i]]\n",
        "print(\"Misclassified Samples (Index, True Label, Predicted Label):\")\n",
        "print(misclassified)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYewx_7eIaIF",
        "outputId": "ce7f1c1d-a91a-488f-f327-f2caf9f1f097"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Misclassified Samples (Index, True Label, Predicted Label):\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier**"
      ],
      "metadata": {
        "id": "yYhriewZIj9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_acc = accuracy_score(y_test, dt.predict(X_test))\n",
        "\n",
        "# Bagging Classifier\n",
        "bagging = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "bag_acc = accuracy_score(y_test, bagging.predict(X_test))\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"Bagging Classifier Accuracy:\", bag_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMZKDWIfIn57",
        "outputId": "843fc07d-706e-4479-e894-6062d7c038ab"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 0.9629629629629629\n",
            "Bagging Classifier Accuracy: 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q35. Train a Random Forest Classifier and visualize the confusion matrix**"
      ],
      "metadata": {
        "id": "If5QDIDxIoZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf.classes_)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "maNuI_DcIrlj",
        "outputId": "2091e1ce-0472-442b-a47d-a0ead4cc0475"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy**"
      ],
      "metadata": {
        "id": "oQuliQXLIr18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Base learners\n",
        "base_learners = [\n",
        "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
        "    ('svm', SVC(probability=True, random_state=42)),\n",
        "    ('lr', LogisticRegression(max_iter=1000, random_state=42))\n",
        "]\n",
        "\n",
        "# Stacking Classifier\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=base_learners,\n",
        "    final_estimator=LogisticRegression(max_iter=1000)\n",
        ")\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Accuracy\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "print(\"Stacking Classifier Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7vNYCnGIw8D",
        "outputId": "abc6a2f7-884e-4969-caf2-ccea0c940dc0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Classifier Accuracy: 0.9814814814814815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q37. Train a Random Forest Classifier and print the top 5 most important features**"
      ],
      "metadata": {
        "id": "v2Nf-1UQIxQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Top 5 features\n",
        "importances = rf.feature_importances_\n",
        "indices = np.argsort(importances)[::-1][:5]\n",
        "\n",
        "print(\"Top 5 Important Features:\")\n",
        "for idx in indices:\n",
        "    print(f\"{load_breast_cancer().feature_names[idx]}: {importances[idx]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8GgCjPWJJTk",
        "outputId": "afcb2fb1-924e-421c-d483-0c73d0013380"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "mean concave points: 0.1419\n",
            "worst concave points: 0.1271\n",
            "worst area: 0.1182\n",
            "mean concavity: 0.0806\n",
            "worst radius: 0.0780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score**"
      ],
      "metadata": {
        "id": "v9k0r1vbJLW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging Classifier\n",
        "bagging = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred = bagging.predict(X_test)\n",
        "\n",
        "# Metrics\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1-score:\", f1_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2zzZfsuJOb3",
        "outputId": "ddea7b97-a9b6-4ed5-8f8f-1aee7e5d0b54"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.963302752293578\n",
            "Recall: 0.9722222222222222\n",
            "F1-score: 0.967741935483871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy**"
      ],
      "metadata": {
        "id": "Os3T9wpSJa3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Compare accuracy with different max_depth\n",
        "for depth in [None, 5, 10, 20]:\n",
        "    rf = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    print(f\"Random Forest (max_depth={depth}) Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73jQhONrJdxE",
        "outputId": "8da3878c-ce7d-40e8-9597-f07b1a4d5ca6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest (max_depth=None) Accuracy: 1.0\n",
            "Random Forest (max_depth=5) Accuracy: 1.0\n",
            "Random Forest (max_depth=10) Accuracy: 1.0\n",
            "Random Forest (max_depth=20) Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance**"
      ],
      "metadata": {
        "id": "OqNwMhv7JgNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Decision Tree base\n",
        "bagging_dt = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "bagging_dt.fit(X_train, y_train)\n",
        "dt_pred = bagging_dt.predict(X_test)\n",
        "dt_mse = mean_squared_error(y_test, dt_pred)\n",
        "\n",
        "# KNN base\n",
        "bagging_knn = BaggingRegressor(estimator=KNeighborsRegressor(), n_estimators=50, random_state=42)\n",
        "bagging_knn.fit(X_train, y_train)\n",
        "knn_pred = bagging_knn.predict(X_test)\n",
        "knn_mse = mean_squared_error(y_test, knn_pred)\n",
        "\n",
        "print(\"Bagging with Decision Tree MSE:\", dt_mse)\n",
        "print(\"Bagging with KNN MSE:\", knn_mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7krA3FeJjHE",
        "outputId": "c0b8158a-7859-4d19-cf9c-90a976129a4c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging with Decision Tree MSE: 0.25787382250585034\n",
            "Bagging with KNN MSE: 1.1020902555745289\n"
          ]
        }
      ]
    }
  ]
}